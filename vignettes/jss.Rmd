---
title:
  formatted: "\\pkg{fairadapt}: Causal Reasoning for Fair Data Pre-processing"
  plain:     "fairadapt: causal reasoning for fair data pre-processing"
  short:     "\\pkg{fairadapt}: Fair Data Adaptation"
author:
  - name: Drago Plecko
    affiliation: ETH Z체rich
    address: >
      Seminar for Statistics
      R채mistrasse 101
      CH-8092 Zurich
    email: \email{drago.plecko@stat.math.ethz.ch}
  - name: Nicolai Meinshausen
    affiliation: ETH Z체rich
    address: >
      Seminar for Statistics
      R채mistrasse 101
      CH-8092 Zurich
    email: \email{meinshausen@stat.math.ethz.ch}
abstract: >
  The abstract of the article.
keywords:
  formatted: [algorithmic fairness, causal inference, machine learning]
  plain:     [algorithmic fairness, causal inference, machine learning]
preamble: >
  \usepackage{amsmath}
  \usepackage{tikz}
  \usepackage{algorithm2e}
  \usepackage{bbm}
  \usepackage{pgfplots}
  \usepackage{array}
  \usepackage{enumerate}
  \usetikzlibrary{arrows.meta}
  \newtheorem{definition}{Definition}
  \newcommand{\pa}{\mathrm{pa}}
  \newcommand{\Pa}{\mathrm{Pa}}
  \newcommand{\de}{\mathrm{de}}
  \newcommand{\ch}{\mathrm{ch}}
  \newcommand{\an}{\mathrm{an}}
  \newcommand{\drago}[1]{{\color{red} Drago: {#1}}}
  \newcommand{\pr}{\mathbbm{P}}
  \renewcommand{\tilde}[1]{ {#1}^{(fp)}}
  \def\ci{{\perp\!\!\!\perp}}
  \pgfmathdeclarefunction{gauss}{2}{\pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}}
vignette: >
  %\VignetteIndexEntry{Fair Data Adaptation (Plecko, JSS 2020)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
output: >
  if (packageVersion("rticles") < 0.5 || rmarkdown::pandoc_version() >= 2)
    rticles::jss_article else rmarkdown::html_vignette
documentclass: jss
bibliography: jss.bib
pkgdown:
  as_is: true
  extension: pdf
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)

library(fairadapt)
library(igraph)
library(data.table)
library(ggplot2)
```
# Introduction
\label{Introduction}

Machine learning is used with increasing frequency in . Algorithms are now used for decision-making in socially sensitive situations, such as predicting credit-score ratings or recidivism during parole. Important early works noted that algorithms are capable of learning societal biases, for example with respect to race \citep{larson2016compas} or gender \citep{blau2003, lambrecht2019algorithmic}. This realization started an important debate in the machine learning community about fairness of algorithms and their impact on decision-making.

The first step of fairness is defining and measuring discrimination. Some inuitive notions have been statistically formalized in order to provide fairness metrics. For example, the notion of demographic parity \citep{darlington1971} requires the protected attribute $A$ (gender/race/religion etc.) to be independent of a constructed classifier or regressor $\widehat{Y}$. Another notion, termed equality of odds \citep{hardt2016}, requires the false positive and false negative rates of classifier $\widehat{Y}$ between different groups (females and males for example), written mathematically as $\widehat{Y} \ci A \mid Y$. To this day, various different notions of fairness exist, which are sometimes incompatible \citep{corbett2018measure}, meaning not of all of them can be achieved for a predictor $\widehat{Y}$ simultaneously. There is no consensus on which notion of fairness is the correct one.

The discussion on algorithmic fairness is, however, not restricted to the machine learning domain. There are many legal and philosophical aspects that have arisen. For example, the legal distinction between disparate impact and disparate treatment \citep{mcginley2011ricci} is important for assessing fairness from a judicial point of view. This in turn emphasizes the importance of the interpretation behind the decision-making process, which is often not the case with black-box machine learning algorithms. For this reason, research in fairness through a causal inference lens has gained more attention.

There are several ways causal inference can help us understand and measure discrimination. The first is counterfactual reasoning \citep{galles1998axiomatic}, which allows us to argue what might have happened under different circumstances which did not ocurr. For example, we might ask whether a female candidate would had been employed, had she been male? This motivated another notions of fairness, termed \textit{counterfactual fairness} \citep{kusner2017counterfactual}, which states that the decision made should stay the same, even if we hypothetically changed someone's race or gender (written succintly as $\widehat{Y}(a) = \widehat{Y}(a')$ in the potential outcome notation). Further, important work has been done in order to decompose the parity gap measure (used for assesing demographic parity), $\pr(\widehat{Y} = 1 \mid A = a) - \pr(\widehat{Y} = 1 \mid A = a')$, into the direct, indirect and spurious components. Lastly, the work of \cite{kilbertus2017avoiding} introduces the so-called resolving variables, in order to relax the possibly prohibitively strong notion of demographic parity. 
This manuscript describes the details of the fair data adaptation method \citep{plecko2020fair}, available on CRAN as the \pkg{fairadapt} package. 

We note that as of the day of writing of the manuscript, there are only 4 CRAN packages related fair machine learning, with \pkg{fairadapt} being the only causal method. Even though many papers on the topic have been published, the fairness domain is still lacking good quality implementations of the existing methods.

The rest of the manuscript is organized as follows. In Section \ref{Method} we describe the methodology behind \pkg{fairadapt}, together with quickly reviewing some of the important concepts of causal inference. In Section \ref{Implementation} we discuss the implementation details and guide the user as to how to use the package. In Section \ref{Illustration} we illustrate the usage of \pkg{fairadapt} by using a large, real-world dataset for a hypothetical fairness application.

# Methodology
\label{Method}

We start by describing the basic idea of \pkg{fairadapt} in a nutshell, followed by the precise mathematical formulation.

## Example: university admission
Consider the following example. Variable $A$ is the protected attribute, in this case gender ($A = a$ corresponding to females, $A = a'$ to males). Let $E$ be educational achievement (measured for example by grades achieved in school) and $T$ the result of an admissions test for further education. Let $Y$ be the outcome of interest (final score) upon which admission to further education is decided. Edges in the graph indicate how variables affect each other.
\begin{center}
		\begin{tikzpicture}
			[>=stealth, rv/.style={circle, draw, thick, minimum size=6mm}, rvc/.style={triangle, draw, thick, minimum size=10mm}, node distance=18mm]
			\pgfsetarrows{latex-latex};
			\begin{scope}
			\node[rv] (1) at (-2,0) {$A$};
			\node[rv] (2) at (0,0) {$E$};
			\node[rv] (3) at (2,0) {$T$};
			\node[rv] (4) at (4,0) {$Y$};
			\draw[->] (1) -- (2);
			\draw[->] (1) edge[bend left = 20] (3);
			\draw[->] (2) -- (3);
			\draw[->] (2) -- (3);
			\draw[->] (3) -- (4);
			\draw[->] (2) edge[bend right = 25] (4);
			\end{scope}
			\end{tikzpicture}
\end{center}
The main problem is that the attribute $A$, gender, has a causal effect on variables $E$, $T$ and $Y$, which we wish to eliminate. For each individual with observed values $(a, e, t, y)$ we want to find a mapping
\[(a, e, t, y) \longrightarrow  (\tilde{a}, \tilde{e}, \tilde{t}, \tilde{y}),\]
which finds the value the person would have obtained in a world where everyone is female. Explicitly, for a male person with education value $e$, we give it the transformed value $\tilde{e}$ chosen such that $$\pr(E \geq e \mid A = a') = \pr(E \geq \tilde{e} \mid A = a). $$
The main idea is that the \textit{relative educational achievement within the subgroup} would stay the same if we changed someone's gender. If you are male and you have a higher educational achievement than 60\% of all males in the dataset, we assume you would be better than 60\% of females had you been female. After computing everyone's education (in the `female' world), we continue by computing the transformed test score values $\tilde{T}$. The approach is again similar, but this time we condition on educational achievement. That is, a male with values $(E, T) = (e, t)$ is assigned a test score $\tilde{t}$ such that
$$\pr(T \geq t \mid E = e, A = a') = \pr(T \geq \tilde{t} \mid E = \tilde{e}, A = a),$$
where the value $\tilde{e}$ was obtained in the previous step. The step can be visualized as follows
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    no markers, domain=0:10, samples=100,
    axis lines*=left, xlabel=$v$, ylabel=density,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=5cm, width=12cm,
    xtick=\empty, ytick=\empty,
    enlargelimits=false, clip=false, axis on top,
    grid = major
    ]
  \addplot [very thick,green!50!black] {gauss(4,1)};
  \addplot [very thick,blue!50!black] {gauss(6.5,0.8)};
  \draw[-{Latex[length=3mm,width=2mm]}, dashed] (axis cs:2.718,0.175) to[bend left = 30] (axis cs:5.474,0.219);
  \draw[-{Latex[length=3mm,width=2mm]}, dashed] (axis cs:4.524401, 0.3476926) to[bend left = 30] (axis cs:6.919520, 0.4346158);

  \node at (axis cs:2.718,0.175) [above, left] {$10\%$ female};
  \node at (axis cs:5.474,0.219) [right] {$10\%$ male};
  \node at (axis cs:4.524401, 0.3476926) [above=0.5cm] {$70\%$ female};
  \node at (axis cs:6.919520, 0.4346158) [right] {$70\%$ male};

  \node at (axis cs:4,0.5) [below = 0.65cm, left = 0.4cm, green!50!black] {$T \geq t \mid E = e, A = a'$};
  \node at (axis cs:6.5,0.5) [below = 1cm, right = 0.9cm, blue!50!black] {$T \mid E = \tilde{e}, A = a$};

  \end{axis}

  \end{tikzpicture}
\end{center}
In the last step, the outcome variable $Y$ needs to be adjusted. The adaptation is based on the values of education and the test score. The transformed value $\tilde{y}$ of $Y = y$ would satisfy
\begin{equation} \label{eq:labeltransform}
	\pr(Y \geq y \mid E = e, T = t, A = a') = \pr(Y \geq \tilde{y} \mid E = \tilde{e}, T = \tilde{t}, A = a).
\end{equation}
This way of counterfactual correction is known as \textit{recursive substitution} \citep[Chapter~7]{pearl2009}.

Before we describe the approach above fully, we introduce an important causal inference concept, related to our discussion:
\begin{definition}
  A structural causal model (SCM) is a 4-tuple $<V, U, \mathcal{F}, P(u)>$, where
  \begin{itemize}
    \item $V = \lbrace V_1, ..., V_n \rbrace$ is the set of observed (endogeneous) variables
    \item $U = \lbrace U_1, ..., U_n \rbrace$ are latent (exogeneous) variables
    \item $\mathcal{F} = \lbrace f_1, ..., f_n \rbrace$ is the set of functions determining $V$, $v_i \gets f_i(\pa(v_i), u_i)$, where $\pa(V_i) \subset V, U_i \subset U$ are the functional arguments of $f_i$
    \item $P(u)$ is a distribution over the exogeneous variables $U$.
  \end{itemize}
\end{definition}
We note that any particular SCM is accompanied by a graphical model $\mathcal{G}$ (a directed acyclic graph), which summarizes which functional arguments are necessary for computing the values of each $V_i$ (that it is, how variables affect each other). We assume throughout, without loss of generality, that
\begin{enumerate}[(i)]
			\item $f_i(\pa(v_i), u_i)$ is increasing in $u_i$ for every fixed $\pa(v_i)$
			\item exogeneous variables $U_i$ are uniformly distributed on $[0, 1]$
\end{enumerate}
We first discuss the so-called Markovian case in which all exogeneous variables $U_i$ are mutually independent.

## Basic formulation - Markovian SCMs

Suppose that $Y$ taking values in $\mathbbm{R}$ is an outcome of interest and $A$ the protected attribute taking two values $a, a'$. Our goal is to describe a pre-processing method which transform the entire data $V$ into its fair version $\tilde{V}$. This is done by computing the counterfactual values $V(A = a)$ which would have been obtained by the individuals, had everyone had the same protected attribute $A = a$.

More precisely, going back to the *university admission* example in equate, we want to "equate" the distributions
\begin{equation}
  V_i \mid \pa(V_i), A = a \text{ and } V_i \mid \pa(V_i), A = a'.
\end{equation}
In words, we want the distribution of $V_i$ to be the same for the female and male applicants, for every variable $V_i$.
Since each function $f_i$ of the original SCM is reparametrized so that $f_i(\pa(v_i), u_i)$ is increasing in $u_i$ for every fixed $\pa(v_i)$, and also that $U_i$ variables are uniformly distributed on $[0, 1]$. Then the $U_i$ variables can be seen as the latent \textit{quantiles}. Our algorithm proceedes as follows:
\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{$V$, causal graph $\mathcal{G}$}
	set $A \gets a$ for everyone\\
	\For{$V_i \in \de(A)$ in topological order}{
	  learn the assignment function $V_i \gets f_i(\pa(V_i), U_i)$ \;
		infer the quantiles $U_i$ associated with the variable $V_i$\;
		transform the values of $V_i$ by using the quantile and the transformed parents (obtained in previous steps)
		$\tilde{V_i} \gets f_i (\tilde{\pa(V_i)}, U_i)$ \;
  }
  \Return{$\tilde{V}$}
	\caption{{\sc Fair Data Adaptation}}
	\label{algo:fairadapt}
\end{algorithm}
The $f_i$ assignment functions of the SCM are of course unknown, but are learned non-parametrically at each step. Notice that Algorithm \ref{algo:fairadapt} is computing the counterfactual values $V(A = a)$ under the $do(A = a)$ intervention for each individual, while keeping the latent quantiles $U$ fixed. In the case of continuous variables, the latent quantiles $U$ can be determined exactly, while for the discrete case, this is more subtle and described in detail in the original fair data adaptation manuscript \citep[Section~5]{plecko2020fair}.

## Adding resolving variables
\citet{kilbertus2017avoiding} discuss that in some situations the protected attribute $A$ can affect variables in a non-discriminatory way. For instance, in the Berkeley admissions dataset \citep{bickel1975sex} we observe that females often apply for departments with lower admission rates and consequently have a lower admission probability. However, we perhaps would not wish to account for this difference in the adaptation procedure if we were to argue that department choice is a choice everybody is free to make. This motivated the following definition:
	\begin{definition}[Resolving variables, \citet{kilbertus2017avoiding}] \label{resolving}
		Let $\mathcal{G}$ be the causal graph of the data generating mechanism. Let the descendants of variable $A$ be denoted by $\de(A)$. A variable $R$ is called resolving if
		\begin{enumerate}[(i)]
			\item $R \in \de(A)$
			\item the causal effect of $A$ on $R$ is considered to be non-discriminatory
\end{enumerate}
\end{definition}
In presence of resolving variables, we compute the counterfactual under a more complicated intervention do$(A = a, R = R(a'))$. The potential outcome value $V(A = a, R = R(a'))$ is obtained by setting $A = a$ and computing the counterfactual while keeping the values of resolving variables to those they \textit{attained naturally}. This is a nested counterfactual and the difference in Algorithm \ref{algo:fairadapt} is simply that resolving variables $R$ are skipped over in the for-loop. 

## Semi-Markovian and topological ordering variant

So far we were concerned with the Markovian case, which assumes that all exogeneous variables $U_i$ are mutually independent. However, in practice this need not be the case. If there are mutual dependencies between the $U_i$s, we are dealing with a so-called Semi-Markovian model. These dependencies between latent variables are represented by dashed, bidirected arrows in the causal diagram. In the university admission example, suppose we had that $U_E \not\!\perp\!\!\!\perp U_T$, meaning that latent variables corresponding to education and test score are correlated. Then the graphical model would be represented as
\begin{center}
	\begin{tikzpicture}
	[>=stealth, rv/.style={circle, draw, thick, minimum size=7mm}, rvc/.style={triangle, draw, thick, minimum size=8mm}, node distance=7mm]
	\pgfsetarrows{latex-latex};
	\begin{scope}
	\node[rv] (a) at (-3,0) {$A$};
	\node[rv] (v1) at (-1,0) {$E$};
	\node[rv] (v2) at (1,0) {$T$};
	\node[rv] (y) at (3,0) {$Y$};
	\draw[->] (a) -- (v1);
	\draw[->] (a) edge[bend left = 30] (v2);
	\draw[->] (v1) -- (v2);
	\draw[->] (v1) edge[bend left = 30] (y);
	\draw[->] (v2) -- (y);
	\path[<->, dashed] (v1) edge[bend right = 20] (v2);
	\end{scope}
	\end{tikzpicture}
\end{center}
There is an important difference in the adaptation procedure for Semi-Markovian case: when inferring the latent quantiles $U_i$ of variable $V_i$, in the Markovian case, only the direct parents $\pa(V_i)$ are needed. In the Semi-Markovian case, due to correlation of latent variables, using only the $\pa(V_i)$ can lead to biased estimates of the $U_i$. Instead, the set of direct parents needs to be extended, described in detail in \ref{tian2002general}. We briefly sketch the argument. Let the \textit{C-components} be a partition of the set $V$, such that all $$\Pa(V_i) := (C(V_i) \cup pa(C(V_i))) \cap \an(V_i),$$
where $\an(V_i)$ are the ancestors of $V_i$. The procedure remains the same as in Algorithm \ref{algo:fairadapt}, with the difference that the set of direct parents $\pa(V_i)$ is replaced by $\Pa(V_i)$ at each step.

## Questions of identifiability

So far, we have not discussed whether it is always possible to do the counterfactual inference described above. In the causal literature, an intervention is \textit{identifiable} if it can be computed uniquely using the data and the assumptions encoded in the graphical model $\mathcal{G}$. The important result by \ref{tian2002general} states that an intervention do$(X = x)$ on a singleton variable $X$ is identifiable if and only if there is not bidirected path between $X$ and $\ch(X)$. Therefore, the intervention is identifiable if

* the model is Markovian
* the model is Semi-Markovian and
 - there is no bidirected path between $A$ and $\ch(A)$, and 
 - there is no bidirected path between $R_i$ and $\ch(R_i)$ for any resolving variable $R_i$.

# Implementation
\label{Implementation}

The implementation is based on the main function `fairadapt()`. We list the most important function arguments and then show these should be specified

* `formula`, argument of type `formula` specifies the dependent and explanatory variables
* `adj.mat`, `cfd.mat` arguments of type `matrix` encode the adjacency and confounding matrices respectively 
* `top.ord` of type `character` is a vector which needs to be specified if `adj.mat` and `cfd.mat` are not known 
* `train.data`, `test.data` of type `data.frame` 
* `protect.A` of type `character` is of length one and names the protected attribute, `res.vars` of type `character` is a vector naming all resolving variables and can vary in length

## Specifying the graphical model

The \pkg{fairadapt} supposes the underlying graphical model $\mathcal{G}$ is known. The model is specified by the adjacency matrix and the confounding matrix. For example, suppose we have the following causal graph $\mathcal{G}$.

For such a graph, we construct the adjacency and confounding matrices and visualize the graph with the `VisualizeGraph()` convenience function that builds on top of the \pkg{igraph} package.

```{r mat, include=T}
adj.mat <- cfd.mat <- array(0, dim = c(4, 4))
colnames(adj.mat) <- rownames(adj.mat) <-
  colnames(cfd.mat) <- rownames(cfd.mat) <- c("A", "E", "T", "Y")

adj.mat["A", c("E", "T")] <-
  adj.mat["E", c("T", "Y")] <-
  adj.mat["T", "Y"] <-
  cfd.mat["E", "T"] <- 
  cfd.mat["T", "E"] <- 1L

toy.graph <- fairadapt:::VisualizeGraph(adj.mat, cfd.mat)
plot(toy.graph, vertex.size = 25, vertex.label.cex = 0.5, 
  vertex.label.color = "black")
```

## A joint training step

We describe the training step using the `fairadapt()` function. We start by constructing a data generator which gives samples from an SCM corresponding to the graph as above
```{r data-gen, include=T}
GenerateData <- function(n) {
  
  epsE <- rnorm(n)
  epsT <- rnorm(n) / sqrt(2) + epsE / sqrt(2)
  
  A <- rbinom(n, 1, 0.5)
  E <- 1 / 2 * A - 1 / 4 + epsE
  T <- 1 / 3 * E + 1 / 3 * A - 2 / 3 + epsT
  Y <- 1 / 3 * E + 1 / 2 * T + rnorm(n)
  
  data.frame(A, E, T, Y)
  
}
```

The `fairadapt()` function works so that training and testing data are given at the same time. The data adaptation is applied to the combination of the two datasets, in order to learn the latent quantiles as precisely as possible (with the exception of label $Y$ which is unavailable on the test set). We note that `train.data` and `test.data` need to have column names which appear in the names of the adjacency matrix `colnames(adj.mat)`. The protected attribute $A$ is given as a character vector `protect.A` of length one.

```{r fairadapt, include=T}
L <- fairadapt(Y ~ ., train.data = GenerateData(100), test.data = GenerateData(100),
  adj.mat = adj.mat, cfd.mat = cfd.mat, protect.A = "A",
  visualize.graph = F, quant.method = "forest")
```

The quantile learning step in Algorithm \ref{algo:fairadapt} can be done using three different methods:
\begin{itemize}
\item Quantile Regression Forests \citep{qrf}
\item Non-crossing quantile neural networks \citep{cannon2018non}
\item Linear Quantile Regression \citep{qr}
\end{itemize}
The summary of the various differences between the methods is given in Table \ref{tab:qmethods}. 

\begin{table}
    \centering
    \begin{tabular}{>{\centering\arraybackslash} m{3.5cm}| >{\centering\arraybackslash} m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm}}
  & Random Forests & Neural Networks & Linear Regression \\ \hline
  \texttt{R}-package & \pkg{ranger} & \pkg{mcqrnn} & \pkg{quantreg} \\ \hline
  \texttt{quant.method} argument & \texttt{"forest"} & \texttt{"nn"} & \texttt{"linear"} \\ \hline
  complexity & $O(n^2\log n)$ & $O(n*?)$ & $O(n)$ \\ \hline
  default parameters & $ntrees = 500$ \newline $mtry = \sqrt{p}$ & 2-layer fully connected perceptron & . \\ \hline
  Time Ex1 & 5 sec & 25 sec & 3 sec \\ \hline
  Time Ex2 & 20 sec & 30 sec & 10 sec \\ \hline
\end{tabular}
    \caption{.}
    \label{tab:qmethods}
\end{table}

## Fair-twin inspection

# Illustration
\label{Illustration}
Here we describe an example of a possible real-world use of \pkg{fairadapt}. Suppose that after a legislative change the US government has decided to adjust the salary of all of its female employees in order to remove both disparate treatment and disparate impact effects. To this end, the government wants to compute the counterfactual salary values of all female employees, that is the salaries that female employees would obtain, had they been male.

To do this, the government is using the from the 2018 American Community Survey by the US Census Bureau. We load the pre-processed version of the dataset:

```{r load-census, include=T}
load("census_gov.RData")
print(head(dat))

# group the columns
protect.A <- "sex"
dmgraph <- c("age", "race", "hispanic_origin", "citizenship", "nativity", 
  "economic_region")
fam <- c("marital", "family_size", "children")
edu <- c("education_level", "english_level")
work <- c("hours_worked", "weeks_worked", "occupation", "industry")
out <- "salary"
```

The hypothesized causal graph for the dataset is given in Figure \ref{fig:censusgraph}. We construct the causal graph and the confounding matrix:

```{r census-graph, include=T, fig.height=5}
col.names <- c(protect.A, dmgraph, fam, edu, work, out)

adj.mat <- cfd.mat <- array(0, dim = c(length(col.names), length(col.names)))
colnames(adj.mat) <- rownames(adj.mat) <- 
  colnames(cfd.mat) <- rownames(cfd.mat) <- 
  col.names

adj.mat[protect.A, c(fam, edu, work, out)] <-
  adj.mat[dmgraph, c(fam, edu, work, out)] <-
  adj.mat[fam, c(edu, work, out)] <-
  adj.mat[edu, c(work, out)] <-
  adj.mat[work, out] <- 
  cfd.mat[protect.A, dmgraph] <- cfd.mat[dmgraph, protect.A] <- 1L

census.graph <- fairadapt:::VisualizeGraph(adj.mat, cfd.mat)
plot(census.graph, vertex.size = 20, vertex.label.cex = 0.5, 
  vertex.label.color = "black")
```

\begin{figure} \centering
	\begin{tikzpicture}
	[>=stealth, rv/.style={circle, draw, thick, minimum size=7mm}, rvc/.style={triangle, draw, thick, minimum size=8mm}, node distance=7mm]
	\pgfsetarrows{latex-latex};
	\begin{scope}
	\node[rv] (c) at (2,2) {$D$};
	\node[rv] (a) at (-2,2) {$A$};
	\node[rv] (m) at (-3,0) {$F$};
	\node[rv] (l) at (-1,0) {$E$};
	\node[rv] (r) at (1,0) {$W$};
	\node[rv] (y) at (3,0) {$Y$};
	\draw[->] (c) -- (m);
	\draw[->] (c) -- (l);
	\draw[->] (c) -- (r);
	\draw[->] (c) -- (y);
	\draw[->] (a) -- (m);
	\draw[->] (m) -- (l);
	\draw[->] (l) -- (r);
	\draw[->] (r) -- (y);
	\path[->] (a) edge[bend left = 0] (l);
	\path[->] (a) edge[bend left = 0] (r);
	\path[->] (a) edge[bend left = 0] (y);
	\path[->] (m) edge[bend right = 20] (r);
	\path[->] (m) edge[bend right = 30] (y);
	\path[->] (r) edge[bend right = 20] (y);
	\path[->, dashed] (a) edge[bend left = 10] (c);
	\end{scope}
	\end{tikzpicture}
	\caption{The causal graph for the Government-Census dataset. $D$ are demographic features, $A$ is gender, $F$ is marital and family information, $E$ education, $W$ work-related information, $Y$ the salary, which is also the outcome of interest.}
	\label{fig:censusgraph}
\end{figure}

Before applying `fairadapt()`, we first log-transform the salaries and look at the densities by gender group

```{r log-and-graph, include=T, fig.height=3}
# log-transform the salaries
dat$salary <- log(dat$salary)

# measure IE, SE, DE
#CausalExplanation(dat[1:10000], protect.A, c(fam, edu, work), dmgraph, out, x0 = "female", x1 = "male")
# plot density before adaptation
nsamples <- 20000

ggplot(dat[1:nsamples], aes(x = salary, fill = sex)) +
  geom_density(alpha = 0.4)  + theme_minimal() + 
  ggtitle("Salary density by gender")
```

There is a clear shift between the two genders, meaning that `male` employees are currently treated better than `female` employees. However, there could be differences in `salary` which are not due to gender inequality, but have to do with the economic region in which the employee works. This needs to be accounted for as well, i.e. the difference between economic regions is not to be removed. To solve the problem, the US governemnt applies \pkg{fairadapt}:

```{r census-adapt, include=T}
L <- fairadapt(salary ~ ., train.data = dat[1:nsamples], test.data = dat[(nsamples+1)],
  adj.mat = adj.mat, protect.A = protect.A, visualize.graph = F)
```

After applying the adaptation, we inspect whether the problem has improved:

```{r vis-adapt, include=T, fig.height=3}
# measure IE, SE, DE again
L[[1]]$sex <- dat[1:nsamples]$sex
#CausalExplanation(L[[1]], protect.A, c(fam, edu, work), dmgraph, out, x0 = "female", x1 = "male")

# plot density after adaptation
ggplot(L[[1]], aes(x = salary, fill = sex)) +
  geom_density(alpha = 0.4)  + theme_minimal() + 
  ggtitle("Adapted salary density by gender")
```

Finally, we can do fair-twin inspection to see how feature values of individual employees have changed:

```{r census-twins, include=T}
inspect.cols <- c("sex", "age", "education_level", "salary")
idx <- sample(which(dat[1:nsamples]$sex == "male"), 5)
print(dat[idx, inspect.cols, with = F])
print(L[[1]][idx, inspect.cols, with = F])
```

Note that `age` does not change, since it is not a descendant of $A$. However, attributes `"education_level"` and `"salary"` do change, as they are descendants of $A$.
